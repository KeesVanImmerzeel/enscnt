library(devtools)
document()
use_data_raw()
install.packages("textclean")
library(textclean)
?replace_word_elongation()
library(enscnt)
document()
use_package(textclean)
use_package("textclean")
use_package("magrittr")
load_all()
?clean_text
clean_text("this is a test")
clean_text("this is a test\n")
?gsub
load_all()
clean_text("this is a test\n")
load_all()
clean_text("this is a test\n")
clean_text("accident_2013.csv.bz2")
?appedn
?append
tw <- readLines("data-raw/en_US.twitter.txt", warn = FALSE)
bl <-  readLines("data-raw/en_US.blogs.txt", warn = FALSE)
ne <- readLines("data-raw/en_US.news.txt", warn = FALSE)
cp <- append(tw,bl,ne)
cp <- append(tw,bl)
cp <- append(tw,bl) %>% append(.,ne)
DATASET <- append(tw,bl) %>% append(.,ne) %>% clean_text(.)
str(DATASET)
str(DATASET[2]
)
str(DATASET[5])
clean_text <- function(text) {
text <- text %>%
replace_word_elongation() %>% ## Replace elongation words
gsub("[^A-Za-z ]", "", .) %>% ## Remove all characters that are not letters or space.
trim()
return(text)
}
clean_text("Words from a complete stranger Made my birthday even better ")
?trim()
clean_text <- function(text) {
text <- text %>%
replace_word_elongation() %>% ## Replace elongation words
gsub("[^A-Za-z ]", "", .) %>% ## Remove all characters that are not letters or space.
trimws()
return(text)
}
clean_text("Words from a complete stranger Made my birthday even better ")
load_all()
usethis::use_data(DATASET, overwrite = TRUE)
str(DATASET)
length(DATASET)
N <- length(DATASET)
n <- length(DATASET)
?sample()
i
i <- sample(1:n,.7*n)
head(i)
min(i)
max(i)
?binomial
rbinom()
?rbinom()
rbinom(10, 2, .1)
rbinom(10, 3, .1)
rbinom(10, 0, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
rbinom(10, 1, .1)
# Select 50% of the lines
i <- sample(n, 1, .5)
head(i)
i
# Select 50% of the lines
i <- rbinom(n, 1, .5)
head(i)
head(i,20)
DATASET <- DATASET[i==1]
length(DATASET)
head(DATASET)
tw <- readLines("data-raw/en_US.twitter.txt", warn = FALSE)
bl <-  readLines("data-raw/en_US.blogs.txt", warn = FALSE)
ne <- readLines("data-raw/en_US.news.txt", warn = FALSE)
DATASET <- append(tw,bl) %>% append(.,ne) %>% clean_text(.)
library(magrittr)
DATASET <- append(tw,bl) %>% append(.,ne) %>% clean_text(.)
library(enscnt)
DATASET <- append(tw,bl) %>% append(.,ne) %>% clean_text(.)
library(devtools)
load_all()
DATASET <- append(tw,bl) %>% append(.,ne)
head(DATASET)
DATASET %>% head(.) %>% clean_text(.)
load_all()
DATASET %>% head(.) %>% clean_text(.)
DATASET %>% head(.)
clean_text <- function(text) {
text <- text %>%
replace_word_elongation() %>% # Replace elongation words
gsub("[^A-Za-z ]", "", .) %>% # Remove all characters that are not letters or space.
gsub("\\s+", " ", .) %>% # Remove multiple white spaces
trimws() %>% # Trim trailing and leading white space
tolower() # Convert to lowercase
return(text)
}
DATASET %>% head(.) %>% clean_text(.)
DATASET %>% tail(.,20) %>% clean_text(.)
?rbinom
i <- rbinom(n = length(DATASET),
size = 1,
prob = .5)
DATASET <- DATASET[i == 1]
usethis::use_data(DATASET, overwrite = TRUE)
scentences
load_all()
?sentences
load_all()
?enscnt
?sentences
load_all()
?sentences
library(devtools)
load_all()
?sentences
??sentences
?enscnt
sentences
library(magrittr)
library(enscnt)
## Prepare `sentences`
tw <- readLines("data-raw/en_US.twitter.txt", warn = FALSE)
bl <-  readLines("data-raw/en_US.blogs.txt", warn = FALSE)
ne <- readLines("data-raw/en_US.news.txt", warn = FALSE)
sentences <- append(tw,bl) %>% append(.,ne) %>% clean_text(.)
# Select 50% of the lines
i <- rbinom(n = length(sentences),
size = 1,
prob = .5)
sentences <- sentences[i == 1]
usethis::use_data(sentences, overwrite = TRUE)
load_all()
?sentences
head(sentences)
df <- data_frame(sentences) %>%
unnest_tokens(word, text) %>%
##anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
mutate(id = row_number()) ## id = the number of frequently used words
library(tibble)
df <- data_frame(sentences) %>%
unnest_tokens(word, text) %>%
##anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
mutate(id = row_number()) ## id = the number of frequently used words
library(tidytext)
install.packages("tidytext")
df <- data_frame(sentences) %>%
unnest_tokens(word, text) %>%
##anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
mutate(id = row_number()) ## id = the number of frequently used words
library(tidytext)
df <- data_frame(sentences) %>%
unnest_tokens(word, text) %>%
##anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
mutate(id = row_number()) ## id = the number of frequently used words
df <- data_frame(sentences[1]) %>%
unnest_tokens(word, text) %>%
##anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
mutate(id = row_number()) ## id = the number of frequently used words
df <- data_frame(sentences[1])
tw <- readLines("data-raw/en_US.twitter.txt", warn = FALSE)
bl <-  readLines("data-raw/en_US.blogs.txt", warn = FALSE)
?unnest_tokens
ne <- readLines("data-raw/en_US.news.txt", warn = FALSE)
sentences <- append(tw,bl) %>% append(.,ne) %>% clean_text(.)
df <- data_frame(sentences) %>%
unnest_tokens(word, text) %>%
##anti_join(stop_words) %>%
count(word, sort = TRUE)
df <- data_frame(sentences[1]) %>%
unnest_tokens(word, text) %>%
##anti_join(stop_words) %>%
count(word, sort = TRUE)
df <- data_frame(sentences[1]) %>%
unnest_tokens(word, text)
df <- data_frame(sentences[1])
df
head(df)
df[2]
library(magrittr)
library(enscnt)
library(tibble)
library(tidytext)
## Prepare `sentences`
tw <- readLines("data-raw/en_US.twitter.txt", warn = FALSE)
bl <-  readLines("data-raw/en_US.blogs.txt", warn = FALSE)
ne <- readLines("data-raw/en_US.news.txt", warn = FALSE)
sentences <- append(tw,bl) %>% append(.,ne)
i <- rbinom(n = length(sentences),
size = 1,
prob = .5)
sentences <- sentences[i == 1]
?writeLines
con <- file("data-raw/en.txt", open="wt")
writeLines(sentences, con)
close(con)
tw <- readLines("data-raw/en_US.twitter.txt", warn = FALSE)
bl <-  readLines("data-raw/en_US.blogs.txt", warn = FALSE)
ne <- readLines("data-raw/en_US.news.txt", warn = FALSE)
sentences <- append(tw,bl) %>% append(.,ne)
i <- rbinom(n = length(sentences),
size = 1,
prob = .25)
sentences <- sentences[i == 1]
con <- file("data-raw/en.txt", open="wt")
writeLines(sentences, con)
close(con)
library(magrittr)
# Prepare a dataset 'data-raw/en.txt' of Englisch sentences drawn from twitter, blogs and newspapers.
# Data is used from the following source
# But actually not included in the package since it is too large.
# https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
# So, before running this code, make sure the following files are present
# in the folder data-raw.
tw <- readLines("data-raw/en_US.twitter.txt", warn = FALSE)
bl <-  readLines("data-raw/en_US.blogs.txt", warn = FALSE)
ne <- readLines("data-raw/en_US.news.txt", warn = FALSE)
sentences <- append(tw,bl) %>% append(.,ne)
# Select 25% of the lines and write the selecte lines to 'data-raw/en.txt'.
i <- rbinom(n = length(sentences),
size = 1,
prob = .25)
sentences <- sentences[i == 1]
con <- file("data-raw/en.txt", open="wt")
writeLines(sentences, con)
close(con)
library(magrittr)
library(enscnt)
library(tibble)
library(tidytext)
## Prepare cleaned dataset `sentences`
sentences <- readLines("data-raw/en.txt", warn = FALSE)
## Prepare cleaned dataset `sentences`
sentences <- readLines("data-raw/en.txt", warn = FALSE) %>% clean_text(.)
library(magrittr)
library(enscnt)
## Prepare cleaned dataset `sentences`
sentences <- readLines("data-raw/en.txt", warn = FALSE) %>% clean_text(.)
usethis::use_data(sentences, overwrite = TRUE)
library(devtools)
document()
load_all()
?sentences
library(devtools)
document()
?ensentences
?ensentences
options(devtools.desc.author=person(given = "Kees",
+                                     family = "van Immerzeel",
+                                     role = c("aut", "cre"),
+                                     email = "c.h.van.immerzeel@gmail.com",
+                                     comment = c(ORCID = "https://orcid.org/0000-0003-0926-3248")))
options(devtools.desc.author=person(given = "Kees", family = "van Immerzeel", role = c("aut", "cre"), email = "c.h.van.immerzeel@gmail.com", comment = c(ORCID = "https://orcid.org/0000-0003-0926-3248")))
options(devtools.desc.license="Artistic-2.0")
document()
?install.packages()
?install.packages("KeesVanImmerzeel/enscnt")
install.packages("KeesVanImmerzeel/enscnt")
install.packages("https://github.com/KeesVanImmerzeel/enscnt")
devtools::install_github("KeesVanImmerzeel/enscnt")
libPath()
devtools::install_github("KeesVanImmerzeel/enscnt")
library(devtools)
document()
library(magrittr)
library(enscnt)
## Prepare cleaned dataset `sentences`
ensentences <- readLines("data-raw/en.txt", warn = FALSE) %>% clean_text(.)
usethis::use_data(ensentences, overwrite = TRUE)
load_all()
devtools::install_github("KeesVanImmerzeel/enscnt")
library(enscnt)
?enscnt
clean_text("8907dsf")
?ensentences
ensenteces[1]
ensentences[1]
?enscnt
?ensentences
